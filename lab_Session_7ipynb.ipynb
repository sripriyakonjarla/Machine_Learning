{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjmVVOBZIhLtkjQ5TwHyOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sripriyakonjarla/Machine_Learning/blob/main/lab_Session_7ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj4b9qmjs9Ct",
        "outputId": "fe9bf417-a594-4d16-ce5b-09d6a1094608"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_excel('training_mathbert.xlsx')\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grids for each classifier\n",
        "param_grids = {\n",
        "    'perceptron': {\n",
        "        'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "        'max_iter': [1000, 2000, 3000],\n",
        "        'tol': [1e-4, 1e-3]\n",
        "    },\n",
        "    'mlp': {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'activation': ['tanh', 'relu'],\n",
        "        'alpha': [0.0001, 0.001, 0.01]\n",
        "    },\n",
        "    'svm': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'decision_tree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'ada_boost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0]\n",
        "    },\n",
        "    'xgboost': {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.3]\n",
        "    },\n",
        "    'naive_bayes': {\n",
        "        'var_smoothing': [1e-9, 1e-8, 1e-7]  # Added var_smoothing parameter\n",
        "    }\n",
        "}\n",
        "\n",
        "def tune_and_evaluate(model, param_grid, X_train, y_train, X_test, y_test):\n",
        "    n_iter = min(10, len(param_grid)) if len(param_grid) > 0 else 1\n",
        "    search = RandomizedSearchCV(model, param_grid, n_iter=n_iter, cv=10, random_state=42, n_jobs=-1)\n",
        "    search.fit(X_train, y_train)\n",
        "    best_model = search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        'best_params': search.best_params_,\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "classifiers = {\n",
        "    'Perceptron': (Perceptron(), param_grids['perceptron']),\n",
        "    'MLP': (MLPClassifier(max_iter=1000), param_grids['mlp']),\n",
        "    'SVM': (SVC(probability=True), param_grids['svm']),\n",
        "    'Decision Tree': (DecisionTreeClassifier(), param_grids['decision_tree']),\n",
        "    'Random Forest': (RandomForestClassifier(), param_grids['random_forest']),\n",
        "    'AdaBoost': (AdaBoostClassifier(algorithm='SAMME'), param_grids['ada_boost']),\n",
        "    'XGBoost': (XGBClassifier(eval_metric='mlogloss'), param_grids['xgboost']),\n",
        "    'Naïve Bayes': (GaussianNB(), param_grids['naive_bayes'])\n",
        "}\n",
        "\n",
        "for name, (model, params) in classifiers.items():\n",
        "    metrics = tune_and_evaluate(model, params, X_train, y_train, X_test, y_test)\n",
        "    metrics['Classifier'] = name\n",
        "    results.append(metrics)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df[['Classifier', 'best_params', 'accuracy', 'precision', 'recall', 'f1_score']]\n",
        "\n",
        "# Styling the results for better appearance\n",
        "styled_results = results_df.style.format({\n",
        "    'accuracy': '{:.2%}',\n",
        "    'precision': '{:.2%}',\n",
        "    'recall': '{:.2%}',\n",
        "    'f1_score': '{:.2%}',\n",
        "}).set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"') \\\n",
        "  .set_properties(**{\n",
        "      'border': '1px solid black',\n",
        "      'text-align': 'center',\n",
        "      'padding': '8px'\n",
        "  }).set_caption(\"Classifier Performance Metrics\") \\\n",
        "  .set_table_styles([\n",
        "      {\n",
        "          'selector': 'thead th',\n",
        "          'props': [('background-color', '#4CAF50'), ('color', 'white')]\n",
        "      },\n",
        "      {\n",
        "          'selector': 'tbody td',\n",
        "          'props': [('border', '1px solid black')]\n",
        "      }\n",
        "  ])\n",
        "\n",
        "# To display in Jupyter Notebook (if applicable)\n",
        "styled_results\n",
        "\n",
        "# If running in a standard Python script, use print\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUK0lea0WqXa",
        "outputId": "c64c9af6-dc6e-4454-a453-c0395511926c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Classifier                                        best_params  accuracy  \\\n",
            "0     Perceptron  {'tol': 0.0001, 'max_iter': 2000, 'alpha': 0.001}  0.849558   \n",
            "1            MLP  {'hidden_layer_sizes': (100,), 'alpha': 0.0001...  0.942478   \n",
            "2            SVM     {'kernel': 'linear', 'gamma': 'auto', 'C': 10}  1.000000   \n",
            "3  Decision Tree  {'min_samples_split': 2, 'min_samples_leaf': 4...  1.000000   \n",
            "4  Random Forest  {'n_estimators': 100, 'min_samples_split': 2, ...  0.964602   \n",
            "5       AdaBoost        {'n_estimators': 100, 'learning_rate': 1.0}  1.000000   \n",
            "6        XGBoost  {'n_estimators': 50, 'max_depth': 3, 'learning...  1.000000   \n",
            "7    Naïve Bayes                           {'var_smoothing': 1e-09}  0.796460   \n",
            "\n",
            "   precision    recall  f1_score  \n",
            "0   0.905156  0.849558  0.858139  \n",
            "1   0.941892  0.942478  0.941969  \n",
            "2   1.000000  1.000000  1.000000  \n",
            "3   1.000000  1.000000  1.000000  \n",
            "4   0.964564  0.964602  0.964178  \n",
            "5   1.000000  1.000000  1.000000  \n",
            "6   1.000000  1.000000  1.000000  \n",
            "7   0.792150  0.796460  0.794023  \n"
          ]
        }
      ]
    }
  ]
}