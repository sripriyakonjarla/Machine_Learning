{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+OAaS1LRQfjEdsipw34a6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sripriyakonjarla/Machine_Learning/blob/main/lab_Session_7ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj4b9qmjs9Ct",
        "outputId": "fe9bf417-a594-4d16-ce5b-09d6a1094608"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_excel('training_mathbert.xlsx')\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grids for each classifier\n",
        "param_grids = {\n",
        "    'perceptron': {\n",
        "        'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "        'max_iter': [1000, 2000, 3000],\n",
        "        'tol': [1e-4, 1e-3]\n",
        "    },\n",
        "    'mlp': {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'activation': ['tanh', 'relu'],\n",
        "        'alpha': [0.0001, 0.001, 0.01]\n",
        "    },\n",
        "    'svm': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'decision_tree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'ada_boost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0]\n",
        "    },\n",
        "    'xgboost': {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.3]\n",
        "    },\n",
        "    'naive_bayes': {\n",
        "        'var_smoothing': [1e-9, 1e-8, 1e-7]  # Added var_smoothing parameter\n",
        "    }\n",
        "}\n",
        "\n",
        "def tune_and_evaluate(model, param_grid, X_train, y_train, X_test, y_test):\n",
        "    n_iter = min(10, len(param_grid)) if len(param_grid) > 0 else 1\n",
        "    search = RandomizedSearchCV(model, param_grid, n_iter=n_iter, cv=10, random_state=42, n_jobs=-1)\n",
        "    search.fit(X_train, y_train)\n",
        "    best_model = search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        'best_params': search.best_params_,\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "classifiers = {\n",
        "    'Perceptron': (Perceptron(), param_grids['perceptron']),\n",
        "    'MLP': (MLPClassifier(max_iter=1000), param_grids['mlp']),\n",
        "    'SVM': (SVC(probability=True), param_grids['svm']),\n",
        "    'Decision Tree': (DecisionTreeClassifier(), param_grids['decision_tree']),\n",
        "    'Random Forest': (RandomForestClassifier(), param_grids['random_forest']),\n",
        "    'AdaBoost': (AdaBoostClassifier(algorithm='SAMME'), param_grids['ada_boost']),\n",
        "    'XGBoost': (XGBClassifier(eval_metric='mlogloss'), param_grids['xgboost']),\n",
        "    'Naïve Bayes': (GaussianNB(), param_grids['naive_bayes'])\n",
        "}\n",
        "\n",
        "for name, (model, params) in classifiers.items():\n",
        "    metrics = tune_and_evaluate(model, params, X_train, y_train, X_test, y_test)\n",
        "    metrics['Classifier'] = name\n",
        "    results.append(metrics)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df[['Classifier', 'best_params', 'accuracy', 'precision', 'recall', 'f1_score']]\n",
        "\n",
        "# To display in Jupyter Notebook (if applicable)\n",
        "styled_results\n",
        "\n",
        "# If running in a standard Python script, use print\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUK0lea0WqXa",
        "outputId": "c64c9af6-dc6e-4454-a453-c0395511926c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Classifier                                        best_params  accuracy  \\\n",
            "0     Perceptron  {'tol': 0.0001, 'max_iter': 2000, 'alpha': 0.001}  0.849558   \n",
            "1            MLP  {'hidden_layer_sizes': (100,), 'alpha': 0.0001...  0.942478   \n",
            "2            SVM     {'kernel': 'linear', 'gamma': 'auto', 'C': 10}  1.000000   \n",
            "3  Decision Tree  {'min_samples_split': 2, 'min_samples_leaf': 4...  1.000000   \n",
            "4  Random Forest  {'n_estimators': 100, 'min_samples_split': 2, ...  0.964602   \n",
            "5       AdaBoost        {'n_estimators': 100, 'learning_rate': 1.0}  1.000000   \n",
            "6        XGBoost  {'n_estimators': 50, 'max_depth': 3, 'learning...  1.000000   \n",
            "7    Naïve Bayes                           {'var_smoothing': 1e-09}  0.796460   \n",
            "\n",
            "   precision    recall  f1_score  \n",
            "0   0.905156  0.849558  0.858139  \n",
            "1   0.941892  0.942478  0.941969  \n",
            "2   1.000000  1.000000  1.000000  \n",
            "3   1.000000  1.000000  1.000000  \n",
            "4   0.964564  0.964602  0.964178  \n",
            "5   1.000000  1.000000  1.000000  \n",
            "6   1.000000  1.000000  1.000000  \n",
            "7   0.792150  0.796460  0.794023  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_excel('training_mathbert.xlsx')\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grids for each classifier\n",
        "param_grids = {\n",
        "    'perceptron': {\n",
        "        'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "        'max_iter': [1000, 2000, 3000],\n",
        "        'tol': [1e-4, 1e-3]\n",
        "    },\n",
        "    'mlp': {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'activation': ['tanh', 'relu'],\n",
        "        'alpha': [0.0001, 0.001, 0.01]\n",
        "    },\n",
        "    'svm': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'decision_tree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'ada_boost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0]\n",
        "    },\n",
        "    'xgboost': {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.3]\n",
        "    },\n",
        "    'naive_bayes': {\n",
        "        'var_smoothing': [1e-9, 1e-8, 1e-7]  # Added var_smoothing parameter\n",
        "    }\n",
        "}\n",
        "\n",
        "def tune_and_evaluate(model, param_grid, X_train, y_train, X_test, y_test):\n",
        "    n_iter = min(10, len(param_grid)) if len(param_grid) > 0 else 1\n",
        "    search = RandomizedSearchCV(model, param_grid, n_iter=n_iter, cv=10, random_state=42, n_jobs=-1)\n",
        "    search.fit(X_train, y_train)\n",
        "    best_model = search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        'best_params': search.best_params_,\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
        "    }\n",
        "\n",
        "results = []\n",
        "hyperparams = []\n",
        "\n",
        "classifiers = {\n",
        "    'Perceptron': (Perceptron(), param_grids['perceptron']),\n",
        "    'MLP': (MLPClassifier(max_iter=1000), param_grids['mlp']),\n",
        "    'SVM': (SVC(probability=True), param_grids['svm']),\n",
        "    'Decision Tree': (DecisionTreeClassifier(), param_grids['decision_tree']),\n",
        "    'Random Forest': (RandomForestClassifier(), param_grids['random_forest']),\n",
        "    'AdaBoost': (AdaBoostClassifier(algorithm='SAMME'), param_grids['ada_boost']),\n",
        "    'XGBoost': (XGBClassifier(eval_metric='mlogloss'), param_grids['xgboost']),\n",
        "    'Naïve Bayes': (GaussianNB(), param_grids['naive_bayes'])\n",
        "}\n",
        "\n",
        "for name, (model, params) in classifiers.items():\n",
        "    metrics = tune_and_evaluate(model, params, X_train, y_train, X_test, y_test)\n",
        "    metrics['Classifier'] = name\n",
        "    results.append(metrics)\n",
        "    hyperparams.append({\n",
        "        'Classifier': name,\n",
        "        'Best Hyperparameters': metrics['best_params']\n",
        "    })\n",
        "\n",
        "# Creating DataFrames for results and hyperparameters\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df[['Classifier', 'accuracy', 'precision', 'recall', 'f1_score']]\n",
        "\n",
        "hyperparams_df = pd.DataFrame(hyperparams)\n",
        "\n",
        "# Displaying the results\n",
        "print(\"Performance Metrics:\")\n",
        "print(results_df)\n",
        "print(\"\\nHyperparameters:\")\n",
        "print(hyperparams_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBiD14k5bjdP",
        "outputId": "5bad0014-19c2-4ed0-e301-e937151de8a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics:\n",
            "      Classifier  accuracy  precision    recall  f1_score\n",
            "0     Perceptron  0.849558   0.905156  0.849558  0.858139\n",
            "1            MLP  0.946903   0.946415  0.946903  0.946267\n",
            "2            SVM  1.000000   1.000000  1.000000  1.000000\n",
            "3  Decision Tree  1.000000   1.000000  1.000000  1.000000\n",
            "4  Random Forest  0.942478   0.943327  0.942478  0.940809\n",
            "5       AdaBoost  1.000000   1.000000  1.000000  1.000000\n",
            "6        XGBoost  1.000000   1.000000  1.000000  1.000000\n",
            "7    Naïve Bayes  0.796460   0.792150  0.796460  0.794023\n",
            "\n",
            "Hyperparameters:\n",
            "      Classifier                               Best Hyperparameters\n",
            "0     Perceptron  {'tol': 0.0001, 'max_iter': 2000, 'alpha': 0.001}\n",
            "1            MLP  {'hidden_layer_sizes': (100,), 'alpha': 0.0001...\n",
            "2            SVM     {'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
            "3  Decision Tree  {'min_samples_split': 2, 'min_samples_leaf': 4...\n",
            "4  Random Forest  {'n_estimators': 100, 'min_samples_split': 2, ...\n",
            "5       AdaBoost        {'n_estimators': 100, 'learning_rate': 1.0}\n",
            "6        XGBoost  {'n_estimators': 50, 'max_depth': 3, 'learning...\n",
            "7    Naïve Bayes                           {'var_smoothing': 1e-09}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_excel('training_mathbert.xlsx')\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grids for each classifier\n",
        "param_grids = {\n",
        "    'perceptron': {\n",
        "        'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "        'max_iter': [1000, 2000, 3000],\n",
        "        'tol': [1e-4, 1e-3]\n",
        "    },\n",
        "    'mlp': {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'activation': ['tanh', 'relu'],\n",
        "        'alpha': [0.0001, 0.001, 0.01]\n",
        "    },\n",
        "    'svm': {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    },\n",
        "    'decision_tree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'max_features': ['sqrt', 'log2', None],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'ada_boost': {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0]\n",
        "    },\n",
        "    'xgboost': {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.3]\n",
        "    },\n",
        "    'naive_bayes': {\n",
        "        'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
        "    }\n",
        "}\n",
        "\n",
        "def tune_and_evaluate(model, param_grid, X_train, y_train, X_test, y_test):\n",
        "    n_iter = min(10, len(param_grid)) if len(param_grid) > 0 else 1\n",
        "    search = RandomizedSearchCV(model, param_grid, n_iter=n_iter, cv=10, random_state=42, n_jobs=-1)\n",
        "    search.fit(X_train, y_train)\n",
        "    best_model = search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        'best_params': search.best_params_,\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, (model, params) in classifiers.items():\n",
        "    metrics = tune_and_evaluate(model, params, X_train, y_train, X_test, y_test)\n",
        "    metrics['Classifier'] = name\n",
        "    results.append(metrics)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Performance Metrics:\")\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}\")\n",
        "    print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {result['precision']:.4f}\")\n",
        "    print(f\"  Recall: {result['recall']:.4f}\")\n",
        "    print(f\"  F1 Score: {result['f1_score']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Print hyperparameters\n",
        "print(\"Hyperparameters:\")\n",
        "for result in results:\n",
        "    print(f\"Classifier: {result['Classifier']}\")\n",
        "    print(f\"  Best Hyperparameters: {result['best_params']}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEDqP781dpmA",
        "outputId": "f7e05a55-e3cd-4de5-f8db-118e46d2e0e5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics:\n",
            "Classifier: Perceptron\n",
            "  Accuracy: 0.8496\n",
            "  Precision: 0.9052\n",
            "  Recall: 0.8496\n",
            "  F1 Score: 0.8581\n",
            "\n",
            "Classifier: MLP\n",
            "  Accuracy: 0.9425\n",
            "  Precision: 0.9419\n",
            "  Recall: 0.9425\n",
            "  F1 Score: 0.9420\n",
            "\n",
            "Classifier: SVM\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1 Score: 1.0000\n",
            "\n",
            "Classifier: Decision Tree\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1 Score: 1.0000\n",
            "\n",
            "Classifier: Random Forest\n",
            "  Accuracy: 0.9823\n",
            "  Precision: 0.9823\n",
            "  Recall: 0.9823\n",
            "  F1 Score: 0.9822\n",
            "\n",
            "Classifier: AdaBoost\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1 Score: 1.0000\n",
            "\n",
            "Classifier: XGBoost\n",
            "  Accuracy: 1.0000\n",
            "  Precision: 1.0000\n",
            "  Recall: 1.0000\n",
            "  F1 Score: 1.0000\n",
            "\n",
            "Classifier: Naïve Bayes\n",
            "  Accuracy: 0.7965\n",
            "  Precision: 0.7921\n",
            "  Recall: 0.7965\n",
            "  F1 Score: 0.7940\n",
            "\n",
            "Hyperparameters:\n",
            "Classifier: Perceptron\n",
            "  Best Hyperparameters: {'tol': 0.0001, 'max_iter': 2000, 'alpha': 0.001}\n",
            "\n",
            "Classifier: MLP\n",
            "  Best Hyperparameters: {'hidden_layer_sizes': (50,), 'alpha': 0.0001, 'activation': 'tanh'}\n",
            "\n",
            "Classifier: SVM\n",
            "  Best Hyperparameters: {'kernel': 'linear', 'gamma': 'auto', 'C': 10}\n",
            "\n",
            "Classifier: Decision Tree\n",
            "  Best Hyperparameters: {'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 30, 'criterion': 'entropy'}\n",
            "\n",
            "Classifier: Random Forest\n",
            "  Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 2, 'max_depth': None}\n",
            "\n",
            "Classifier: AdaBoost\n",
            "  Best Hyperparameters: {'n_estimators': 100, 'learning_rate': 1.0}\n",
            "\n",
            "Classifier: XGBoost\n",
            "  Best Hyperparameters: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.01}\n",
            "\n",
            "Classifier: Naïve Bayes\n",
            "  Best Hyperparameters: {'var_smoothing': 1e-09}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}