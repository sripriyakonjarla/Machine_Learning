{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBNohxPvU2Oa4ifudBfphO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sripriyakonjarla/Machine_Learning/blob/main/tuning_bart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartModel\n",
        "\n",
        "# Load the BART tokenizer and model\n",
        "model_name = \"facebook/bart-base\"  # You can choose 'facebook/bart-base' or 'facebook/bart-large'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartModel.from_pretrained(model_name)\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file_path = 'testingData2.xlsx'  # Replace with your file path\n",
        "df = pd.read_excel(excel_file_path)\n",
        "\n",
        "# Assuming the answers are in a column named 'input'\n",
        "answers = df['Input'].tolist()  # Adjust the column name as needed\n",
        "\n",
        "# Function to generate embeddings using BART\n",
        "def generate_embeddings(texts):\n",
        "    texts = [str(text) for text in texts]\n",
        "    # Tokenize the input texts\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Generate embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Get the last hidden states (embeddings)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "\n",
        "    # Mean embedding for each text\n",
        "    mean_embeddings = embeddings.mean(dim=1)\n",
        "    return mean_embeddings\n",
        "\n",
        "# Process in batches to avoid memory issues\n",
        "batch_size = 16\n",
        "all_embeddings = []\n",
        "for i in range(0, len(answers), batch_size):\n",
        "    batch_answers = answers[i:i + batch_size]\n",
        "    embeddings = generate_embeddings(batch_answers)\n",
        "    all_embeddings.append(embeddings)\n",
        "\n",
        "# Concatenate all embeddings into a single tensor\n",
        "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "# Convert to a DataFrame for easier saving\n",
        "embeddings_df = pd.DataFrame(all_embeddings.numpy())\n",
        "\n",
        "# Save embeddings to a new Excel file\n",
        "embeddings_df.to_excel('testing_embeddings.xlsx', index=False)  # Adjust the save path as needed\n",
        "print(\"Embeddings generated and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9flTmmT9q0f",
        "outputId": "e7b7262d-4a74-4806-f190-4c1cc050bfa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g36dHUvSxxr8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier  # Example classifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset from an Excel file\n",
        "data = pd.read_excel('bart_embeddings.xlsx')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target variable\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PumdZhE_zhR3",
        "outputId": "4f539353-2817-417f-9f05-7b4a7eedbf35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"SVM\": SVC(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"CatBoost\": CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=0),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to hold metrics\n",
        "metrics = {model_name: {} for model_name in models.keys()}\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_scaled, y)\n",
        "    y_pred = model.predict(X_scaled)\n",
        "\n",
        "    metrics[model_name]['Accuracy'] = accuracy_score(y, y_pred)\n",
        "    metrics[model_name]['Precision'] = precision_score(y, y_pred, average='weighted')\n",
        "    metrics[model_name]['Recall'] = recall_score(y, y_pred, average='weighted')\n",
        "    metrics[model_name]['F1 Score'] = f1_score(y, y_pred, average='weighted')\n",
        "\n",
        "# Display metrics\n",
        "for model_name, model_metrics in metrics.items():\n",
        "    print(f\"{model_name}: {model_metrics}\")"
      ],
      "metadata": {
        "id": "WKUX_ccdyDPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c45e0a9-be7e-4f78-aa82-86c9143cc7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM: {'Accuracy': 0.8053571428571429, 'Precision': 0.8093708704359247, 'Recall': 0.8053571428571429, 'F1 Score': 0.8058759372323357}\n",
            "KNN: {'Accuracy': 0.756547619047619, 'Precision': 0.7567349396097982, 'Recall': 0.756547619047619, 'F1 Score': 0.7565872206031347}\n",
            "Decision Tree: {'Accuracy': 0.9791666666666666, 'Precision': 0.9795091545818201, 'Recall': 0.9791666666666666, 'F1 Score': 0.9791168193041022}\n",
            "Gaussian Naive Bayes: {'Accuracy': 0.5369047619047619, 'Precision': 0.564503007601679, 'Recall': 0.5369047619047619, 'F1 Score': 0.5260989623311423}\n",
            "Random Forest: {'Accuracy': 0.9791666666666666, 'Precision': 0.9792658791997, 'Recall': 0.9791666666666666, 'F1 Score': 0.979191341907501}\n",
            "Logistic Regression: {'Accuracy': 0.9321428571428572, 'Precision': 0.9321610767299515, 'Recall': 0.9321428571428572, 'F1 Score': 0.9321450170150424}\n",
            "MLP: {'Accuracy': 0.9648809523809524, 'Precision': 0.9648745872835096, 'Recall': 0.9648809523809524, 'F1 Score': 0.9648772290495987}\n",
            "XGBoost: {'Accuracy': 0.9791666666666666, 'Precision': 0.9793306933644451, 'Recall': 0.9791666666666666, 'F1 Score': 0.9791811136688023}\n",
            "CatBoost: {'Accuracy': 0.9791666666666666, 'Precision': 0.9793617303156656, 'Recall': 0.9791666666666666, 'F1 Score': 0.9791989666143436}\n",
            "AdaBoost: {'Accuracy': 0.6916666666666667, 'Precision': 0.6977385403048453, 'Recall': 0.6916666666666667, 'F1 Score': 0.6935174327068164}\n",
            "Extra Trees: {'Accuracy': 0.9791666666666666, 'Precision': 0.9795091545818201, 'Recall': 0.9791666666666666, 'F1 Score': 0.9791168193041022}\n",
            "Gradient Boosting: {'Accuracy': 0.9422619047619047, 'Precision': 0.9425401585614185, 'Recall': 0.9422619047619047, 'F1 Score': 0.9423286442040691}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import sys  # For flushing output\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"SVM\": SVC(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"CatBoost\": CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=0),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to hold metrics\n",
        "metrics = {model_name: {} for model_name in models.keys()}\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_scaled, y)\n",
        "    y_pred = model.predict(X_scaled)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision = precision_score(y, y_pred, average='weighted')\n",
        "    recall = recall_score(y, y_pred, average='weighted')\n",
        "    f1 = f1_score(y, y_pred, average='weighted')\n",
        "\n",
        "    # Store metrics and hyperparameters\n",
        "    metrics[model_name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'Hyperparameters': model.get_params()\n",
        "    }\n",
        "\n",
        "    # Print results immediately after evaluation\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Hyperparameters: {metrics[model_name]['Hyperparameters']}\")\n",
        "    print(f\"  Accuracy: {metrics[model_name]['Accuracy']}\")\n",
        "    print(f\"  Precision: {metrics[model_name]['Precision']}\")\n",
        "    print(f\"  Recall: {metrics[model_name]['Recall']}\")\n",
        "    print(f\"  F1 Score: {metrics[model_name]['F1 Score']}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Flush to make sure the output is immediately displayed\n",
        "    sys.stdout.flush()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr1A7xaCYZdp",
        "outputId": "d6aff25d-998a-4933-a7f5-61d0d0f367a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM:\n",
            "  Hyperparameters: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
            "  Accuracy: 0.8053571428571429\n",
            "  Precision: 0.8093708704359247\n",
            "  Recall: 0.8053571428571429\n",
            "  F1 Score: 0.8058759372323357\n",
            "--------------------------------------------------------------------------------\n",
            "KNN:\n",
            "  Hyperparameters: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
            "  Accuracy: 0.756547619047619\n",
            "  Precision: 0.7567349396097982\n",
            "  Recall: 0.756547619047619\n",
            "  F1 Score: 0.7565872206031347\n",
            "--------------------------------------------------------------------------------\n",
            "Decision Tree:\n",
            "  Hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}\n",
            "  Accuracy: 0.9791666666666666\n",
            "  Precision: 0.9795091545818201\n",
            "  Recall: 0.9791666666666666\n",
            "  F1 Score: 0.9791168193041022\n",
            "--------------------------------------------------------------------------------\n",
            "Gaussian Naive Bayes:\n",
            "  Hyperparameters: {'priors': None, 'var_smoothing': 1e-09}\n",
            "  Accuracy: 0.5369047619047619\n",
            "  Precision: 0.564503007601679\n",
            "  Recall: 0.5369047619047619\n",
            "  F1 Score: 0.5260989623311423\n",
            "--------------------------------------------------------------------------------\n",
            "Random Forest:\n",
            "  Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
            "  Accuracy: 0.9791666666666666\n",
            "  Precision: 0.9791700665108545\n",
            "  Recall: 0.9791666666666666\n",
            "  F1 Score: 0.9791588983134413\n",
            "--------------------------------------------------------------------------------\n",
            "Logistic Regression:\n",
            "  Hyperparameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
            "  Accuracy: 0.9321428571428572\n",
            "  Precision: 0.9321610767299515\n",
            "  Recall: 0.9321428571428572\n",
            "  F1 Score: 0.9321450170150424\n",
            "--------------------------------------------------------------------------------\n",
            "MLP:\n",
            "  Hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
            "  Accuracy: 0.9648809523809524\n",
            "  Precision: 0.9648745872835096\n",
            "  Recall: 0.9648809523809524\n",
            "  F1 Score: 0.9648772290495987\n",
            "--------------------------------------------------------------------------------\n",
            "XGBoost:\n",
            "  Hyperparameters: {'objective': 'multi:softprob', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
            "  Accuracy: 0.9791666666666666\n",
            "  Precision: 0.9793306933644451\n",
            "  Recall: 0.9791666666666666\n",
            "  F1 Score: 0.9791811136688023\n",
            "--------------------------------------------------------------------------------\n",
            "CatBoost:\n",
            "  Hyperparameters: {'iterations': 1000, 'learning_rate': 0.1, 'depth': 6, 'verbose': 0}\n",
            "  Accuracy: 0.9791666666666666\n",
            "  Precision: 0.9793617303156656\n",
            "  Recall: 0.9791666666666666\n",
            "  F1 Score: 0.9791989666143436\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost:\n",
            "  Hyperparameters: {'algorithm': 'SAMME.R', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 100, 'random_state': 42}\n",
            "  Accuracy: 0.6916666666666667\n",
            "  Precision: 0.6977385403048453\n",
            "  Recall: 0.6916666666666667\n",
            "  F1 Score: 0.6935174327068164\n",
            "--------------------------------------------------------------------------------\n",
            "Extra Trees:\n",
            "  Hyperparameters: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
            "  Accuracy: 0.9791666666666666\n",
            "  Precision: 0.9795091545818201\n",
            "  Recall: 0.9791666666666666\n",
            "  F1 Score: 0.9791168193041022\n",
            "--------------------------------------------------------------------------------\n",
            "Gradient Boosting:\n",
            "  Hyperparameters: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 42, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
            "  Accuracy: 0.9422619047619047\n",
            "  Precision: 0.9425401585614185\n",
            "  Recall: 0.9422619047619047\n",
            "  F1 Score: 0.9423286442040691\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import sys  # For flushing output\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"SVM\": SVC(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"CatBoost\": CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=0),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to hold metrics\n",
        "metrics = {model_name: {} for model_name in models.keys()}\n",
        "cv_folds = 5\n",
        "\n",
        "# Evaluate each model using cross-validation\n",
        "for model_name, model in models.items():\n",
        "    # Perform cross-validation for each metric\n",
        "    accuracy_scores = cross_val_score(model, X_scaled, y, cv=cv_folds, scoring='accuracy')\n",
        "    precision_scores = cross_val_score(model, X_scaled, y, cv=cv_folds, scoring='precision_weighted')\n",
        "    recall_scores = cross_val_score(model, X_scaled, y, cv=cv_folds, scoring='recall_weighted')\n",
        "    f1_scores = cross_val_score(model, X_scaled, y, cv=cv_folds, scoring='f1_weighted')\n",
        "\n",
        "    # Calculate mean and standard deviation for each metric\n",
        "    metrics[model_name] = {\n",
        "        'Accuracy Mean': np.mean(accuracy_scores),\n",
        "        'Accuracy Std': np.std(accuracy_scores),\n",
        "        'Precision Mean': np.mean(precision_scores),\n",
        "        'Precision Std': np.std(precision_scores),\n",
        "        'Recall Mean': np.mean(recall_scores),\n",
        "        'Recall Std': np.std(recall_scores),\n",
        "        'F1 Score Mean': np.mean(f1_scores),\n",
        "        'F1 Score Std': np.std(f1_scores),\n",
        "        'Hyperparameters': model.get_params()\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Hyperparameters: {metrics[model_name]['Hyperparameters']}\")\n",
        "    print(f\"  Accuracy Mean: {metrics[model_name]['Accuracy Mean']:.4f}, Accuracy Std: {metrics[model_name]['Accuracy Std']:.4f}\")\n",
        "    print(f\"  Precision Mean: {metrics[model_name]['Precision Mean']:.4f}, Precision Std: {metrics[model_name]['Precision Std']:.4f}\")\n",
        "    print(f\"  Recall Mean: {metrics[model_name]['Recall Mean']:.4f}, Recall Std: {metrics[model_name]['Recall Std']:.4f}\")\n",
        "    print(f\"  F1 Score Mean: {metrics[model_name]['F1 Score Mean']:.4f}, F1 Score Std: {metrics[model_name]['F1 Score Std']:.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Flush to make sure the output is immediately displayed\n",
        "    sys.stdout.flush()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SLHD1_qY-T50",
        "outputId": "5580c6f8-c172-49db-da14-ea4d56c3d66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM:\n",
            "  Hyperparameters: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
            "  Accuracy Mean: 0.5268, Accuracy Std: 0.0418\n",
            "  Precision Mean: 0.5343, Precision Std: 0.0425\n",
            "  Recall Mean: 0.5268, Recall Std: 0.0418\n",
            "  F1 Score Mean: 0.5243, F1 Score Std: 0.0395\n",
            "--------------------------------------------------------------------------------\n",
            "KNN:\n",
            "  Hyperparameters: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
            "  Accuracy Mean: 0.4821, Accuracy Std: 0.0270\n",
            "  Precision Mean: 0.4902, Precision Std: 0.0261\n",
            "  Recall Mean: 0.4821, Recall Std: 0.0270\n",
            "  F1 Score Mean: 0.4822, F1 Score Std: 0.0263\n",
            "--------------------------------------------------------------------------------\n",
            "Decision Tree:\n",
            "  Hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}\n",
            "  Accuracy Mean: 0.4589, Accuracy Std: 0.0247\n",
            "  Precision Mean: 0.4672, Precision Std: 0.0152\n",
            "  Recall Mean: 0.4542, Recall Std: 0.0227\n",
            "  F1 Score Mean: 0.4495, F1 Score Std: 0.0170\n",
            "--------------------------------------------------------------------------------\n",
            "Gaussian Naive Bayes:\n",
            "  Hyperparameters: {'priors': None, 'var_smoothing': 1e-09}\n",
            "  Accuracy Mean: 0.4702, Accuracy Std: 0.0597\n",
            "  Precision Mean: 0.4757, Precision Std: 0.0547\n",
            "  Recall Mean: 0.4702, Recall Std: 0.0597\n",
            "  F1 Score Mean: 0.4513, F1 Score Std: 0.0497\n",
            "--------------------------------------------------------------------------------\n",
            "Random Forest:\n",
            "  Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
            "  Accuracy Mean: 0.5274, Accuracy Std: 0.0445\n",
            "  Precision Mean: 0.5232, Precision Std: 0.0313\n",
            "  Recall Mean: 0.5167, Recall Std: 0.0415\n",
            "  F1 Score Mean: 0.5196, F1 Score Std: 0.0451\n",
            "--------------------------------------------------------------------------------\n",
            "Logistic Regression:\n",
            "  Hyperparameters: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
            "  Accuracy Mean: 0.5000, Accuracy Std: 0.0258\n",
            "  Precision Mean: 0.5018, Precision Std: 0.0235\n",
            "  Recall Mean: 0.5000, Recall Std: 0.0258\n",
            "  F1 Score Mean: 0.4977, F1 Score Std: 0.0275\n",
            "--------------------------------------------------------------------------------\n",
            "MLP:\n",
            "  Hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
            "  Accuracy Mean: 0.5351, Accuracy Std: 0.0407\n",
            "  Precision Mean: 0.5382, Precision Std: 0.0391\n",
            "  Recall Mean: 0.5351, Recall Std: 0.0407\n",
            "  F1 Score Mean: 0.5330, F1 Score Std: 0.0428\n",
            "--------------------------------------------------------------------------------\n",
            "XGBoost:\n",
            "  Hyperparameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
            "  Accuracy Mean: 0.5226, Accuracy Std: 0.0418\n",
            "  Precision Mean: 0.5277, Precision Std: 0.0395\n",
            "  Recall Mean: 0.5226, Recall Std: 0.0418\n",
            "  F1 Score Mean: 0.5217, F1 Score Std: 0.0405\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5aff590e4cf1>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Perform cross-validation for each metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0maccuracy_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprecision_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precision_weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mrecall_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'recall_weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    424\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5243\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5245\u001b[0;31m         self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[1;32m   5246\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5247\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset from an Excel file\n",
        "data = pd.read_excel('bart_embeddings.xlsx')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target variable\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Base models for stacking\n",
        "# Base models for stacking\n",
        "base_learners = [\n",
        "    ('svm', SVC(kernel='rbf', gamma='scale', degree=4, C=10)),\n",
        "    ('knn', KNeighborsClassifier(weights='uniform', n_neighbors=7, metric='manhattan')),\n",
        "    ('dt', DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=4, max_features='sqrt', max_depth=25, criterion='gini')),\n",
        "    ('gnb', GaussianNB(var_smoothing=1e-09)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, min_samples_split=10, min_samples_leaf=1, max_features='sqrt', max_depth=20, class_weight='balanced', bootstrap=True)),\n",
        "    ('logreg', LogisticRegression(penalty='l2', C=0.01)),\n",
        "    ('mlp', MLPClassifier(solver='adam', learning_rate='constant', hidden_layer_sizes=(50,), activation='logistic', max_iter=500, verbose=True)),\n",
        "    ('catboost', cb.CatBoostClassifier(learning_rate=0.2, iterations=200, depth=4, verbose=0)),\n",
        "    ('xgboost', xgb.XGBClassifier(subsample=0.8, n_estimators=200, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0)),\n",
        "    ('extratrees', ExtraTreesClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', bootstrap=True)),\n",
        "    ('gb', GradientBoostingClassifier(subsample=1.0, n_estimators=200, max_features='sqrt', max_depth=6, learning_rate=0.3))\n",
        "]\n",
        "\n",
        "# Meta-classifier for stacking\n",
        "meta_model = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
        "\n",
        "# Create the Stacking Classifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5  # Cross-validation within the training data itself\n",
        ")\n",
        "\n",
        "# Define the metrics you want to evaluate during cross-validation\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'f1': make_scorer(f1_score)\n",
        "}\n",
        "\n",
        "# Perform cross-validation on the full dataset (training data only)\n",
        "cv_results = cross_validate(stacking_model, X_scaled, y, cv=5, scoring=scoring)\n",
        "\n",
        "# Print the cross-validation results for each metric\n",
        "print(f\"Cross-validation accuracy: {cv_results['test_accuracy'].mean():.4f} ± {cv_results['test_accuracy'].std():.4f}\")\n",
        "print(f\"Cross-validation precision: {cv_results['test_precision'].mean():.4f} ± {cv_results['test_precision'].std():.4f}\")\n",
        "print(f\"Cross-validation recall: {cv_results['test_recall'].mean():.4f} ± {cv_results['test_recall'].std():.4f}\")\n",
        "print(f\"Cross-validation F1-score: {cv_results['test_f1'].mean():.4f} ± {cv_results['test_f1'].std():.4f}\")"
      ],
      "metadata": {
        "id": "i-EknYUhNwSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "\n",
        "# Load your dataset (adjust path to your actual dataset)\n",
        "df_test = pd.read_excel('testing_embeddings.xlsx')\n",
        "\n",
        "# Assuming the test data contains columns 'X' for features and 'y' for labels (adjust column names as needed)\n",
        "X_test = df_test.drop(columns=['output'])  # Adjust to your actual feature columns\n",
        "y_test = df_test['output']  # Adjust to your actual target column\n",
        "\n",
        "# Define the best hyperparameters (as you've mentioned in the question)\n",
        "models = {\n",
        "    \"SVM\": SVC(kernel='rbf', gamma='scale', degree=4, C=10),\n",
        "    \"KNN\": KNeighborsClassifier(weights='uniform', n_neighbors=7, metric='manhattan'),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(min_samples_split=2, min_samples_leaf=4, max_features='sqrt', max_depth=25, criterion='gini'),\n",
        "    \"GaussianNB\": GaussianNB(var_smoothing=1e-09),\n",
        "    \"RF\": RandomForestClassifier(n_estimators=100, min_samples_split=10, min_samples_leaf=1, max_features='sqrt', max_depth=20, class_weight='balanced', bootstrap=True),\n",
        "    \"Logistic Regression\": LogisticRegression(penalty='l2', C=0.01),\n",
        "    \"MLP\": MLPClassifier(solver='adam', learning_rate='constant', hidden_layer_sizes=(50,), activation='logistic', max_iter=500),\n",
        "    \"CatBoost\": cb.CatBoostClassifier(learning_rate=0.2, iterations=200, depth=4),\n",
        "    \"XGBoost\": xgb.XGBClassifier(subsample=0.8, n_estimators=200, max_depth=7, learning_rate=0.1, gamma=1, colsample_bytree=1.0, class_weight='balanced'),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, learning_rate=1.0),\n",
        "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', max_depth=None, bootstrap=True),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(subsample=1.0, n_estimators=200, max_features='sqrt', max_depth=6, learning_rate=0.3)\n",
        "}\n",
        "\n",
        "# Dictionary to store the actual and predicted results\n",
        "results = {}\n",
        "\n",
        "# Train and predict with each model\n",
        "for model_name, model in models.items():\n",
        "    # Fit the model to the training data (you can adjust the training data as needed)\n",
        "    model.fit(X_test, y_test)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate confusion matrix and accuracy\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store the results for each model\n",
        "    results[model_name] = {\n",
        "        \"Actual\": y_test.tolist(),\n",
        "        \"Predicted\": y_pred.tolist(),\n",
        "        \"Confusion Matrix\": cm,\n",
        "        \"Accuracy\": accuracy\n",
        "    }\n",
        "\n",
        "# Print results for each model\n",
        "for model_name, result in results.items():\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "\n",
        "    # Create a DataFrame to show Actual vs Predicted values in a table format\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Actual': result[\"Actual\"],\n",
        "        'Predicted': result[\"Predicted\"]\n",
        "    })\n",
        "\n",
        "    print(comparison_df)  # Print the actual vs predicted values for the current model\n",
        "    print(f\"Accuracy: {result['Accuracy']}\")\n",
        "    print(f\"Confusion Matrix:\\n{result['Confusion Matrix']}\")\n",
        "    print(\"\\n\" + \"-\"*50)  # Just a separator for clarity between models\n"
      ],
      "metadata": {
        "id": "ndqVw486FTAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}