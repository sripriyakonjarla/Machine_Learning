{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvvFLC+qR2QyDkG1cXOZu4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sripriyakonjarla/Machine_Learning/blob/main/gpt2_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# Load the dataset from an Excel file\n",
        "data = pd.read_excel('gpt2_embeddings.xlsx')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]    # Target variable\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate"
      ],
      "metadata": {
        "id": "y3cgAi-SMHDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "models = {\n",
        "    \"SVM\": SVC(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"MLP\": MLPClassifier(max_iter=1000, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
        "    \"CatBoost\": CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=0),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "}\n",
        "\n",
        "# Evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    results[name] = {\n",
        "        'Accuracy': round(accuracy, 4),\n",
        "        'Precision': round(precision, 4),\n",
        "        'Recall': round(recall, 4),\n",
        "        'F1 Score': round(f1, 4)\n",
        "    }\n",
        "\n",
        "# Print the results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJF_sBkhtDGZ",
        "outputId": "59eee01a-94bb-4790-946d-12e43dd2bd82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: SVM\n",
            "  Accuracy: 0.5893\n",
            "  Precision: 0.5997\n",
            "  Recall: 0.5802\n",
            "  F1 Score: 0.5855\n",
            "Model: KNN\n",
            "  Accuracy: 0.5863\n",
            "  Precision: 0.5912\n",
            "  Recall: 0.5811\n",
            "  F1 Score: 0.5842\n",
            "Model: Decision Tree\n",
            "  Accuracy: 0.4554\n",
            "  Precision: 0.4499\n",
            "  Recall: 0.4464\n",
            "  F1 Score: 0.4477\n",
            "Model: Gaussian Naive Bayes\n",
            "  Accuracy: 0.5268\n",
            "  Precision: 0.5293\n",
            "  Recall: 0.539\n",
            "  F1 Score: 0.5274\n",
            "Model: Random Forest\n",
            "  Accuracy: 0.5714\n",
            "  Precision: 0.582\n",
            "  Recall: 0.5564\n",
            "  F1 Score: 0.5621\n",
            "Model: Logistic Regression\n",
            "  Accuracy: 0.5625\n",
            "  Precision: 0.5699\n",
            "  Recall: 0.5599\n",
            "  F1 Score: 0.5628\n",
            "Model: MLP\n",
            "  Accuracy: 0.5804\n",
            "  Precision: 0.5883\n",
            "  Recall: 0.577\n",
            "  F1 Score: 0.5806\n",
            "Model: XGBoost\n",
            "  Accuracy: 0.5744\n",
            "  Precision: 0.5848\n",
            "  Recall: 0.5609\n",
            "  F1 Score: 0.5669\n",
            "Model: CatBoost\n",
            "  Accuracy: 0.5952\n",
            "  Precision: 0.6089\n",
            "  Recall: 0.5839\n",
            "  F1 Score: 0.5904\n",
            "Model: AdaBoost\n",
            "  Accuracy: 0.5506\n",
            "  Precision: 0.5607\n",
            "  Recall: 0.5369\n",
            "  F1 Score: 0.5427\n",
            "Model: Extra Trees\n",
            "  Accuracy: 0.5685\n",
            "  Precision: 0.5773\n",
            "  Recall: 0.5581\n",
            "  F1 Score: 0.5637\n",
            "Model: Gradient Boosting\n",
            "  Accuracy: 0.5982\n",
            "  Precision: 0.6026\n",
            "  Recall: 0.5872\n",
            "  F1 Score: 0.5919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7LPwWYvfz3Y",
        "outputId": "7336a9dc-ca6a-482d-c166-a09278a9a833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8jiMhWldPcr",
        "outputId": "2b04930c-b5a8-421d-eb69-8204e30c2a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'weights': 'distance', 'n_neighbors': 15, 'metric': 'minkowski'}\n",
            "Evaluation Metrics: {'Accuracy Mean': 0.6079, 'Accuracy STD': 0.0392, 'Precision Mean': 0.6175, 'Precision STD': 0.0394, 'Recall Mean': 0.6098, 'Recall STD': 0.0419, 'F1 Mean': 0.6102, 'F1 STD': 0.0397}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define the kNN model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=knn,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=10,  # 10-fold cross-validation\n",
        "    scoring='f1_macro',  # Change this to any other scoring metric if needed\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model to find the best parameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_knn = random_search.best_estimator_\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# Perform cross-validation on the best model using StratifiedKFold\n",
        "cv_score = cross_validate(best_knn, X_train, y_train, cv=cv,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "\n",
        "# Define the Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier()\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=gb_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=10,  # 10-fold cross-validation\n",
        "    scoring='f1_macro',  # Change this to any other scoring metric if needed\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model to find the best parameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_gb = random_search.best_estimator_\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# Perform cross-validation on the best model using StratifiedKFold\n",
        "cv_score = cross_validate(best_gb, X_train, y_train, cv=cv,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rTNRRu0My5s",
        "outputId": "046f5491-018d-40ed-818a-e5df97553b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "70 fits failed out of a total of 200.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "7 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "63 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.60590761 0.61463197 0.61898716 0.53931424 0.61730695\n",
            " 0.47221308 0.61625222 0.62815767 0.63190705        nan 0.62270928\n",
            "        nan 0.61866271 0.62770661        nan 0.62128277        nan\n",
            "        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'subsample': 0.8, 'n_estimators': 200, 'max_features': 'log2', 'max_depth': 4, 'learning_rate': 0.2}\n",
            "Evaluation Metrics: {'Accuracy Mean': 0.6146, 'Accuracy STD': 0.0251, 'Precision Mean': 0.6237, 'Precision STD': 0.0242, 'Recall Mean': 0.6165, 'Recall STD': 0.0255, 'F1 Mean': 0.6175, 'F1 STD': 0.0232}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "70 fits failed out of a total of 200.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "53 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "17 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.60851574 0.62661382 0.63065867 0.52686003 0.61796867\n",
            " 0.46691505 0.61193509 0.61566182 0.61217373        nan 0.63135578\n",
            "        nan 0.61608909 0.6194465         nan 0.60894109        nan\n",
            "        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'subsample': 0.8, 'n_estimators': 200, 'max_features': 'log2', 'max_depth': 3, 'learning_rate': 0.1}\n",
            "Evaluation Metrics: {'Accuracy Mean': 0.6273, 'Accuracy STD': 0.0277, 'Precision Mean': 0.6359, 'Precision STD': 0.0271, 'Recall Mean': 0.6263, 'Recall STD': 0.0276, 'F1 Mean': 0.6287, 'F1 STD': 0.0264}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "\n",
        "# Define the Extra Trees model\n",
        "extra_trees_model = ExtraTreesClassifier()\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [False, True]\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=extra_trees_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=10,  # 10-fold cross-validation\n",
        "    scoring='f1_macro',  # Change this to any other scoring metric if needed\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model to find the best parameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_extra_trees = random_search.best_estimator_\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# Perform cross-validation on the best model using StratifiedKFold\n",
        "cv_score = cross_validate(best_extra_trees, X_train, y_train, cv=cv,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OWOXJlYEM1TF",
        "outputId": "3b761c2a-a142-486a-9927-33f216dc3fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-1c113014dec5>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Fit the model to find the best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Output the best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             ParameterSampler(\n\u001b[1;32m   1962\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb = GaussianNB()\n",
        "\n",
        "# No hyperparameters to tune, so we can fit and evaluate directly\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Perform cross-validation on the model\n",
        "cv_score = cross_validate(nb, X_train, y_train, cv=10, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Step 1: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Step 2: Handle class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Step 3: Train the Naive Bayes model\n",
        "nb = GaussianNB(var_smoothing=1e-9)  # Apply Laplace smoothing\n",
        "nb.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Step 4: Evaluate using Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "cv_score = cross_validate(nb, X_train_resampled, y_train_resampled, cv=skf,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "print(\"Naive Bayes Evaluation Metrics:\", results)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-jYx5gIdxA1",
        "outputId": "fb1fa36c-2530-4306-fa30-c742d1939b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Evaluation Metrics: {'Accuracy Mean': 0.5403, 'Accuracy STD': 0.0224, 'Precision Mean': 0.5359, 'Precision STD': 0.0218, 'Recall Mean': 0.5403, 'Recall STD': 0.0224, 'F1 Mean': 0.5337, 'F1 STD': 0.023}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5,10],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt'],\n",
        "    'bootstrap': [True,False],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=5,\n",
        "    cv=StratifiedKFold(n_splits=5),\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and score\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf = random_search.best_estimator_\n",
        "\n",
        "# Perform cross-validation on the best Random Forest model\n",
        "cv_score = cross_validate(best_rf, X_train, y_train, cv=5,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWf4lhewdzK4",
        "outputId": "e0e0f5ef-5b91-48b4-e798-e636af716b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'class_weight': 'balanced', 'bootstrap': True}\n",
            "Evaluation Metrics: {'Accuracy Mean': 0.6161, 'Accuracy STD': 0.0169, 'Precision Mean': 0.6221, 'Precision STD': 0.0149, 'Recall Mean': 0.6202, 'Recall STD': 0.0187, 'F1 Mean': 0.6197, 'F1 STD': 0.016}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.01, 0.1, 1.0, 10.0],\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=5,\n",
        "    cv=10,\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Perform cross-validation on the best model\n",
        "best_log_reg = random_search.best_estimator_\n",
        "cv_score = cross_validate(best_log_reg, X_train, y_train, cv=10, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "print(\"Logistic Regression Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRiKHmhvd09p",
        "outputId": "2c52f123-8c81-40a8-d055-7567e7badd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Best Parameters: {'penalty': 'l2', 'C': 0.01}\n",
            "Logistic Regression Evaluation Metrics: {'Accuracy Mean': 0.6034, 'Accuracy STD': 0.0336, 'Precision Mean': 0.6035, 'Precision STD': 0.0334, 'Recall Mean': 0.6102, 'Recall STD': 0.0358, 'F1 Mean': 0.6044, 'F1 STD': 0.0328}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset from an Excel file\n",
        "data = pd.read_excel('gpt2_embeddings.xlsx')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]    # Target variable\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Step 1: Feature Scaling (important for many models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Step 2: Handle class imbalance using SMOTE (if needed)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Step 3: Define the AdaBoost model with a shallow decision tree as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=base_estimator,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of boosting rounds (trees)\n",
        "    'learning_rate': [0.01, 0.1, 1.0],  # How much each estimator contributes\n",
        "    'estimator__max_depth': [3, 5, 7]  # Max depth of the decision tree (base estimator)\n",
        "}\n",
        "\n",
        "# Step 5: Set up RandomizedSearchCV with StratifiedKFold for cross-validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=ada,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=5,\n",
        "    cv=skf,\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Step 6: Fit the model with the best parameters\n",
        "random_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"AdaBoost Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Step 7: Perform cross-validation on the best AdaBoost model\n",
        "best_ada = random_search.best_estimator_\n",
        "cv_score = cross_validate(\n",
        "    best_ada, X_train_resampled, y_train_resampled, cv=skf, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False\n",
        ")\n",
        "# Step 8: Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"AdaBoost Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "id": "ovI_LbVxd3lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('gpt2_embeddings.xlsx')\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target\n",
        "\n",
        "# Scale the features\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Apply PCA to retain 95% of variance\n",
        "pca = PCA(n_components=0.9999)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "Js-tZzt-Lh0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'degree': [2, 3, 4]  # Applicable only for polynomial kernel\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=svm,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=10,  # 10-fold cross-validation\n",
        "    scoring='f1_macro',  # Change this to any other scoring metric if needed\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all available cores\n",
        ")\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Get the best estimator\n",
        "best_svm = random_search.best_estimator_\n",
        "\n",
        "# Perform cross-validation on the best model\n",
        "cv_score = cross_validate(best_svm, X_train, y_train, cv=10, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9xab1GZ5_Li",
        "outputId": "209938a2-f29b-4cae-f627-ea1f8f9dad33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'kernel': 'rbf', 'gamma': 'auto', 'degree': 3, 'C': 1}\n",
            "Evaluation Metrics: {'Accuracy Mean': 0.6192, 'Accuracy STD': 0.0517, 'Precision Mean': 0.6345, 'Precision STD': 0.0544, 'Recall Mean': 0.616, 'Recall STD': 0.0521, 'F1 Mean': 0.6217, 'F1 STD': 0.0534}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "\n",
        "# Define the Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier()\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=gb_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=10,  # 10-fold cross-validation\n",
        "    scoring='f1_macro',  # Change this to any other scoring metric if needed\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model to find the best parameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_gb = random_search.best_estimator_\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# Perform cross-validation on the best model using StratifiedKFold\n",
        "cv_score = cross_validate(best_gb, X_train, y_train, cv=cv,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "id": "8yRq1E6WQHrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "\n",
        "# Define the Extra Trees model\n",
        "extra_trees_model = ExtraTreesClassifier()\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [False, True]\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=extra_trees_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=10,  # 10-fold cross-validation\n",
        "    scoring='f1_macro',  # Change this to any other scoring metric if needed\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model to find the best parameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_extra_trees = random_search.best_estimator_\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# Perform cross-validation on the best model using StratifiedKFold\n",
        "cv_score = cross_validate(best_extra_trees, X_train, y_train, cv=cv,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "id": "ZcN7kcdpQKEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the kNN model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=knn,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=10,  # 10-fold cross-validation\n",
        "    scoring='f1_macro',  # Change this to any other scoring metric if needed\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model to find the best parameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_knn = random_search.best_estimator_\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# Perform cross-validation on the best model using StratifiedKFold\n",
        "cv_score = cross_validate(best_knn, X_train, y_train, cv=cv,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=True)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Training Accuracy Mean': round(cv_score['train_accuracy'].mean(), 4),  # Added training accuracy\n",
        "    'Training Accuracy STD': round(cv_score['train_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5ErwVCpL__O",
        "outputId": "8dabeee4-c154-410b-eed1-c3f06dc918a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'weights': 'uniform', 'n_neighbors': 5, 'metric': 'euclidean'}\n",
            "Evaluation Metrics: {'Accuracy Mean': 0.6118, 'Accuracy STD': 0.0584, 'Training Accuracy Mean': 0.7319, 'Training Accuracy STD': 0.0093, 'Precision Mean': 0.6179, 'Precision STD': 0.061, 'Recall Mean': 0.6128, 'Recall STD': 0.0583, 'F1 Mean': 0.6133, 'F1 STD': 0.0593}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Define the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'degree': [2, 3, 4]  # Applicable only for polynomial kernel\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=svm,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of random combinations to try\n",
        "    cv=10,  # 10-fold cross-validation\n",
        "    scoring='f1_macro',  # Change this to any other scoring metric if needed\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all available cores\n",
        ")\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_svm = random_search.best_estimator_\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# Perform cross-validation on the best model using StratifiedKFold\n",
        "cv_score = cross_validate(best_svm, X_train, y_train, cv=cv,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=True)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Training Accuracy Mean': round(cv_score['train_accuracy'].mean(), 4),  # Added training accuracy\n",
        "    'Training Accuracy STD': round(cv_score['train_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)"
      ],
      "metadata": {
        "id": "fZ88P4h7fRS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "\n",
        "nb = GaussianNB()\n",
        "\n",
        "# No hyperparameters to tune, so we can fit and evaluate directly\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Perform cross-validation on the model\n",
        "cv_score = cross_validate(nb, X_train, y_train, cv=10, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Step 1: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Step 2: Handle class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Step 3: Train the Naive Bayes model\n",
        "nb = GaussianNB(var_smoothing=1e-9)  # Apply Laplace smoothing\n",
        "nb.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Step 4: Evaluate using Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "cv_score = cross_validate(nb, X_train_resampled, y_train_resampled, cv=skf,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "                          return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "print(\"Naive Bayes Evaluation Metrics:\", results)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFrhWA-icSRR",
        "outputId": "a527d74f-82d9-4a10-e851-2bd42219b33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Evaluation Metrics: {'Accuracy Mean': 0.4694, 'Accuracy STD': 0.0419, 'Precision Mean': 0.4764, 'Precision STD': 0.0425, 'Recall Mean': 0.4694, 'Recall STD': 0.0424, 'F1 Mean': 0.444, 'F1 STD': 0.0415}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5,10],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt'],\n",
        "    'bootstrap': [True,False],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    cv=StratifiedKFold(n_splits=10),\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and score\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_rf = random_search.best_estimator_\n",
        "\n",
        "# Perform cross-validation on the best Random Forest model\n",
        "cv_score = cross_validate(best_rf, X_train, y_train, cv=10,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "id": "g8doRJ0tdpsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7259314d-72b1-4b61-ed1f-0c52119b911a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'class_weight': 'balanced', 'bootstrap': False}\n",
            "Evaluation Metrics: {'Accuracy Mean': 0.6035, 'Accuracy STD': 0.0419, 'Precision Mean': 0.6086, 'Precision STD': 0.0424, 'Recall Mean': 0.6043, 'Recall STD': 0.0431, 'F1 Mean': 0.6047, 'F1 STD': 0.0423}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.01, 0.1, 1.0, 10.0],\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    cv=10,\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Perform cross-validation on the best model\n",
        "best_log_reg = random_search.best_estimator_\n",
        "cv_score = cross_validate(best_log_reg, X_train, y_train, cv=10, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "\n",
        "# Collect and round the results\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "print(\"Logistic Regression Evaluation Metrics:\", results)\n"
      ],
      "metadata": {
        "id": "u_08rQ4qgh-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE  # For handling class imbalance\n",
        "\n",
        "# Optional: Scaling the features (this isn't strictly necessary for Decision Trees but may help)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Optional: Handling class imbalance using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Define the Decision Tree model\n",
        "dtree = DecisionTreeClassifier(random_state=42, class_weight='balanced')  # Use class_weight='balanced' to handle class imbalance\n",
        "\n",
        "# Set up the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 5, 10, 15, 20, 25],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 4, 6, 10],\n",
        "    'max_features': [None, 'sqrt', 'log2'],\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=dtree,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    cv=StratifiedKFold(n_splits=10),\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_dtree = random_search.best_estimator_\n",
        "\n",
        "cv_score = cross_validate(best_dtree, X_train_resampled, y_train_resampled, cv=StratifiedKFold(n_splits=10),\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "print(\"Evaluation Metrics:\",results)"
      ],
      "metadata": {
        "id": "CE0p40DYhLz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(max_iter=2000, random_state=42)\n",
        "\n",
        "param_distributions = {\n",
        "    'hidden_layer_sizes': [(50,), (100,)],\n",
        "    'activation': ['logistic', 'tanh', 'relu'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=mlp,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    cv=10,\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"MLP Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_mlp = random_search.best_estimator_\n",
        "cv_score = cross_validate(best_mlp, X_train, y_train, cv=10, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "print(\"MLP Evaluation Metrics:\",results)"
      ],
      "metadata": {
        "id": "wlJs0hWZhRFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8030f2-fac1-4a3e-82be-2ddf73bc9959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Best Parameters: {'solver': 'sgd', 'learning_rate': 'constant', 'hidden_layer_sizes': (100,), 'activation': 'relu'}\n",
            "MLP Evaluation Metrics: {'Accuracy Mean': 0.6199, 'Accuracy STD': 0.0417, 'Precision Mean': 0.6245, 'Precision STD': 0.0424, 'Recall Mean': 0.623, 'Recall STD': 0.0419, 'F1 Mean': 0.6219, 'F1 STD': 0.0419}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "xgb = XGBClassifier(random_state=42)\n",
        "\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01,  0.1],\n",
        "    'subsample': [ 0.8, 1.0],\n",
        "    'colsample_bytree': [ 0.8, 1.0],\n",
        "    'gamma': [0, 1],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    cv=StratifiedKFold(n_splits=10),\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_xgb = random_search.best_estimator_\n",
        "\n",
        "cv_score = cross_validate(best_xgb, X_train, y_train, cv=10,\n",
        "                          scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False)\n",
        "\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "print(\"Evaluation Metrics:\",results)"
      ],
      "metadata": {
        "id": "WL1hU8OIyYCM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "0458b2cd-9d92-4c40-b881-5542a2550530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e2e97e8bd7b2>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             ParameterSampler(\n\u001b[1;32m   1962\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "base_estimator = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "ada = AdaBoostClassifier(\n",
        "    estimator=base_estimator,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "param_distributions = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "    'estimator__max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=ada,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=10,\n",
        "    cv=skf,\n",
        "    scoring='f1_macro',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"AdaBoost Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "best_ada = random_search.best_estimator_\n",
        "cv_score = cross_validate(\n",
        "    best_ada, X_train_resampled, y_train_resampled, cv=skf, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], return_train_score=False\n",
        ")\n",
        "results = {\n",
        "    'Accuracy Mean': round(cv_score['test_accuracy'].mean(), 4),\n",
        "    'Accuracy STD': round(cv_score['test_accuracy'].std(), 4),\n",
        "    'Precision Mean': round(cv_score['test_precision_macro'].mean(), 4),\n",
        "    'Precision STD': round(cv_score['test_precision_macro'].std(), 4),\n",
        "    'Recall Mean': round(cv_score['test_recall_macro'].mean(), 4),\n",
        "    'Recall STD': round(cv_score['test_recall_macro'].std(), 4),\n",
        "    'F1 Mean': round(cv_score['test_f1_macro'].mean(), 4),\n",
        "    'F1 STD': round(cv_score['test_f1_macro'].std(), 4),\n",
        "}\n",
        "\n",
        "print(\"AdaBoost Evaluation Metrics:\",results)"
      ],
      "metadata": {
        "id": "kqVsCI7Wht8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac2c4ee2-0fa9-4eb0-c464-242e3071c6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Best Parameters: {'n_estimators': 200, 'learning_rate': 1.0, 'estimator__max_depth': 5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Evaluation Metrics: {'Accuracy Mean': 0.642, 'Accuracy STD': 0.0797, 'Precision Mean': 0.6611, 'Precision STD': 0.0681, 'Recall Mean': 0.6421, 'Recall STD': 0.0797, 'F1 Mean': 0.6443, 'F1 STD': 0.0744}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IvcHar_VhudM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}